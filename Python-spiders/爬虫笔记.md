1.第三方库的requests模块的使用

requsts模块是python3引入的第三方库外援(很好用)

(1)requests.get()方法的使用，使用pip安装即可

首先我们要导入这个模块
```python
import requests
```
然后需要个网址
```python
url = "https://www.baidu.com/"
```
直接使用
```python
r = requests.get ( url )
```
然后我们试着输出一下结果
```python
print ( r )#这个输出  <Response [200]>
print ( type ( r ) )# <class 'requests.models.Response'>  他是这个类型
print ( r.text ) #这里打印的是网页源码
print ( type ( r.text ) ) #字符串类型
```
到这里大概了解了requests.get()方法的大概用法

(2)request.get()的第二个参数，params
```python
params = {} #这里的参数必须是一个字典
r = requests.get ( url , params )
```
这样就能把参数导进去，比如百度的界面，将参数进行设置，就能直接跳转到参数设定的搜索结果的界面
比如
```python
params = { 'key1': 'value1' , 'key2': 'value2' }
r = requests.get ( url , params )
print ( r.url ) #  一打印得到  https://www.baidu.com/?key2=value2&key1=value1
```
(3)requests还自带了一个json解析器，可以解析json数据
```python
r = requests.get ( url , params ).json()
```
这在解析动态异步加载的界面就很方便，使用r = requests.get ( url , params ).json().get()就可以直接查找需要获得的数据

(4)伪装浏览器

有的网站，具有反爬虫机制，会自动检测出你是不是爬虫，这个时候，就可以给自己的爬虫添加一个请求头headers将自己伪装成浏览器

headers有很多，在百度上一搜就有很多
```python
headers = {'user-agent': 'my-app/0.0.1'} #这只是其中一种
r = requests.get( url , headers = headers ) 
```
这样就可以了

2.使用Xpath抓取你想要的东西

xpath其实就是通过路径找到你想要的东西，比如说你家住在胜利路牛逼小区a幢1单元1楼1号，通过这个我就能找到你，放在Xpath上，它就是通过选取XML文档的节点来一层一层的找到你想要的内容。

使用xpath时要用到 lxml包里的etree模块 直接用pip下载

直接用上面requests返回的结果
```python
s = etree.HTML( r.text )
```
(1)绝对路径和相对路径的区别

在了解Xpath语法之前，需要先弄清楚绝对路径和相对路径。

绝对路径，故名思意，绝对路径就是必须一层一层的严格按照结构来。比如我要找你的话，我必须要通过 a国b省c市d区e街f小区g单元h楼i号，一层也不能少。

但是如果全世界都只有一个小区叫牛逼小区，我们就不用前面的所有国，省，市，直接在牛逼小区中找g单元h楼i号。

放在电脑路径中，比如在D盘中有个test文件夹里又有a,b两个文件夹两个文件夹里都有c这个文件，a中还有d文件。

绝对路径D:\a\c这样找到a中的c文件 如果用相对路径找d D:\\d

这里就用双斜杠，这就是绝对路径和相对路径的区别

注意：相对路径可以获取所有满足条件的东西，譬如，这里我们使用 D:\\c 这个就可以得到两个c文件

(2)xpath的一些简单的语法

a.选取div节点中a中的所有元素(xpath返回的结果都是 列表 )
```python
list = s.xpath('//div//a/text()') #一般要加text()
```
b.选取div节点中属性class为abc的结果
```python
list = s.xpath('//div[@class="abc"]/text()')
```
c.选取div节点中，a节点的href属性的值
```python
list = s.xpath('//div//a/@href')
```
平时经常用到的就是这些了

**注意：当获取的是标签的属性时就不加/text()，在获取标签的内容时就要加上text()

3.BeautifulSoup的基本用法

其实有一种也就差不多了，有时候因为xpath功力不够，有的东西抓不到，不妨就试试用另外一种解析方式，也就是BeautifulSoup,这是bs4中的一个模块。

(1)使用是先要导入BeautifulSoup这个模块 
```python
from bs4 import BeautifulSoup
```
(2)BeautifulSoup是可以和requests模块一起使用的
```python
url = "https://www.baidu.com/"
r = requests.get( url )
r.encoding = 'utf-8'
bs = BeautifulSoup( r.text , "html.parser" ) #后边的参数是解析器，有多种，html.parser是python自带的
```
我们可以打印一下
```python
print ( bs ) #这个打印出来是网页源码
print ( type ( bs ) ) #这个打印是出来是<class 'bs4.BeautifulSoup'> 是bs4里定义的类 
```
(3)获取标签

比如获取P标签直接就是
```python
print ( bs.p ) #这里打印出来就是整个p标签，包括其标签头和标签内容，以及其中的属性
print ( bs.p.string ) #获取标签内容
print ( bs.p.name )#获取标签名称 就是p
print ( bs.p.parent.name ) #获取父节点的名称
```
但是这些方法都只能得到从上到下的第一个节点，如果想要获取到整个网站上的某一类就要用到下面的方法

(4)find和find_all 方法

find方法其实在爬虫上面没什么卵用，主要是find_all方法的用法

比如我们要获取一个网址上所有的a标签
```python
List = bs.find_all ( 'a' ) 
```
这样我们就得到了所有a标签

但是有的a标签并不是我们想要的，比如我们只想要class 属性为abc的a标签
```python
List = bs.find_all ( 'a' , class_ = 'abc') #这里注意，因为class是Python的关键字，所以为了避免电脑瓜皮，要加个_
```
再进一步，如果我们要的是a标签中某个属性的值 ,比如href属性
```python
List2 = []
for i in List :
List2.append( i.get ( 'href' ) ) 
```
这样就将所需要的东西放到了List2 中

通过循环遍历所有标签:分为上序遍历,下序遍历,平行便利
</br>先煮汤
```python
r = requets.get( url )
soup = beautifulsoup( r.text , "html.parese" )
```
* 上序遍历(假设a标签是底层标签)
```python
for parent in soup.a.parent:
    print ( parent )
```
* 下序遍历(html为顶层标签)
```python
for children in soup.html.children
    print ( children )
```
